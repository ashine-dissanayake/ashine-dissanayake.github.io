# Chapter 4: Natural Language (NLP)

## What is NLP?
- Language Model: Having read the words before, the model will predict the words yet to come. 
- Also considered a self-supervised model. Labels not required but rather head also of test, sentences and words. The model must be able to understand the specified English.
- **Note:** Self-supervised Model. No labels provided however they are gathered from the independent variables. 
- **Example:** Language model pretrained on Wikipedia but applied to IMDB. 

## ULMFit Approach
- The additional step of fine-tuning the language model, prior to transfer learning to a classification task, resulted in significantly better predictions.
- There are three stages for transfer language in NLP. 

![](/images/uml-fit.png "fast.ai's logo")

## Text Processing 
- **Primary Problem:** All sentences have different lengths and documents can be very long. How can the next word be predicted? 
- The approach taken for a categorical variable was to: 
  - Return a list of all categorical variables. 
  - Replace level with index. 
  - Row = level. Create matrix. 
  - This ,matrix will be A LAYER in you NN. 
 
 ## Tokenisation 
 - **Objective** is to convert test into a sequence of words (i.e., word, character and substring).
 - Three main approaches. 
  - **Word-based:** Split sentence at spaces. Applying language specific rules. Punctuation marks are also split into separate tokens.
  - **Subword based:** Split into smaller parts, where the most common substring appear. 
  - **Character-based:** Split a sentence into its individual characters.

 ## Numericalization
 Make a list of all of the unique words that appear and convert each word into a UID, by looking up its index in the vocab.
 
## Questionnaire
1. What is "self-supervised learning"?
1. What is a "language model"?
1. Why is a language model considered self-supervised?
1. What are self-supervised models usually used for?
1. Why do we fine-tune language models?
1. What are the three steps to create a state-of-the-art text classifier?
1. How do the 50,000 unlabeled movie reviews help us create a better text classifier for the IMDb dataset?
1. What are the three steps to prepare your data for a language model?
1. What is "tokenization"? Why do we need it?
1. Name three different approaches to tokenization.
1. What is xxbos?
1. List four rules that fastai applies to text during tokenization.
1. Why are repeated characters replaced with a token showing the number of repetitions and the character that's repeated?
1. What is "numericalization"?
1. Why might there be words that are replaced with the "unknown word" token?
1. With a batch size of 64, the first row of the tensor representing the first batch contains the first 64 tokens for the dataset. What does the second row of that tensor contain? What does the first row of the second batch contain? (Carefulâ€”students often get this one wrong! Be sure to check your answer on the book's website.)
1. Why do we need padding for text classification? Why don't we need it for language modeling?
1. What does an embedding matrix for NLP contain? What is its shape?
1. What is "perplexity"?
1. Why do we have to pass the vocabulary of the language model to the classifier data block?
1. What is "gradual unfreezing"?
1. Why is text generation always likely to be ahead of automatic identification of machine-generated texts?
